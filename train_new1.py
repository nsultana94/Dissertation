# -*- coding: utf-8 -*-
"""conv lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pk_9y1txgv_KprxWLG0C-PkaUjAF-E3J

# Installation
"""

import logging 
logging.basicConfig(filename="std4.log", 
					format='%(asctime)s %(message)s', 
					filemode='w')

logger=logging.getLogger() 
logger.setLevel(logging.DEBUG) 
import torch 
import cv2
import random
import numpy as np 

import glob
import matplotlib.pyplot as plt
import pandas as pd
from networks import SegmentationModel, ConvLSTMCell, UnetInitialiser2, LSTMModel2
from dataloader import get_train_augs, get_test_augs, get_valid_augs, SegmentationDataset
from train_function1 import train_function, eval_function
DEVICE = torch.device('cuda') 
# DEVICE = 'cuda' #Cuda as using GPU



EPOCHS = 50 #25 training iterations
LR = 0.00001 #decay learning rate
BATCH_SIZE = 4
HEIGHT = 288
WIDTH = 480
ENCODER = 'resnet34'
WEIGHTS = 'imagenet'
DATA_URL = "/cs/student/projects1/2019/nsultana/"


training_images = (glob.glob(f"{DATA_URL}updated_data/Data/Training/*.npz"))
print(len(training_images))

testing_images = (glob.glob(f"{DATA_URL}updated_data/Data/Testing/*.npz"))
print(len(testing_images))
validation_images = (glob.glob(f"{DATA_URL}updated_data/Data/Validation/*.npz"))
print(len(validation_images))

"""# Set up model"""


model = SegmentationModel()
model = model.to(device =DEVICE); #i.e CUDA


"""# Set up dataset and data loader"""



trainset = SegmentationDataset("Training", get_train_augs(), training_images)
validset = SegmentationDataset("Validation", get_valid_augs(), validation_images)
testset = SegmentationDataset("Testing", get_test_augs(), testing_images)

from torch.utils.data import DataLoader
trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True,num_workers=2) #every epoch batches shuffles
validloader = DataLoader(validset, batch_size = BATCH_SIZE, shuffle = True,num_workers=2)




"""# Training model"""

model.load_state_dict(torch.load(f'{DATA_URL}Models/best_model_aug.pt'))
model_summary = model.show()
encoder = model_summary.encoder
decoder = model_summary.decoder
head = model_summary.segmentation_head


convlstm = ConvLSTMCell(input_size = 512, hidden_size = 512)

initialiser = UnetInitialiser2(encoder)
initialiser = initialiser.to(device = DEVICE)

# initialiser.load_state_dict(torch.load(f'{DATA_URL}Models/U-net/testing_kernel_combo_1x1.pt'))


new_model = LSTMModel2(initialiser, encoder,convlstm,decoder, head)
new_model = new_model.to(device = DEVICE)

encoder = new_model.encoder 
decoder = new_model.decoder




for name,param in initialiser.named_parameters():
   param.requires_grad = False

for name,param in encoder.named_parameters():
   param.requires_grad = False

for name,param in decoder.named_parameters():
   param.requires_grad = False
  
params_to_train = ['initializer.decoder.blocks.4.conv2.0.weight', 'initializer.decoder.blocks.4.conv2.1.weight', 'initializer.decoder.blocks.4.conv2.1.bias', 'initializer.decoder.blocks.4.conv1.0.weight', 'initializer.decoder.blocks.4.conv1.1.weight', 'initializer.decoder.blocks.4.conv1.1.bias']


for name,param in head.named_parameters():
  param.requires_grad = False

for name,param in new_model.named_parameters():
    if name in params_to_train:
        param.requires_grad = False
    logger.info(f"{name}, {param.requires_grad}")

epoch_start = 0

best_valid_loss = np.inf

EPOCHS = 100


valid_losses = []
train_losses = []



LR = 0.00001
optimizer = torch.optim.Adam(new_model.parameters(), lr = LR)
# lambda1 = lambda1 = lambda epoch : pow((1 - epoch / EPOCHS), 0.9)
# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,lambda1)



number_epoch_to_save = 5
 
counter = 0

print(epoch_start, best_valid_loss)



for epoch in range(epoch_start,EPOCHS):


    train_loss = train_function(trainloader, new_model, optimizer)
    valid_loss = eval_function(validloader, new_model)

  
  
    if valid_loss < best_valid_loss: #if best valid loss then upate new model
        torch.save(new_model.state_dict(), f'{DATA_URL}Models/encoder_as_initialiser.pt')
        print("Saved model")
        logger.info(f"Saved model: {valid_loss} - {epoch}") 
        best_valid_loss = valid_loss
        counter = 0

   

    if counter > 20:
        print(f"Early stopping: {epoch}, best valid loss : {best_valid_loss}")
        logger.info(f"Early stopping") 
        break
    
    
        

    if epoch % number_epoch_to_save == 0:

        torch.save({
                    'epoch': epoch,
                    'model_state_dict': new_model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': valid_loss,
                    'best_loss': best_valid_loss,
                    'train_loss': train_loss,
                    'train_losses': train_losses,
                    'valid_losses': valid_losses
                    }, f'{DATA_URL}Models/lstm_unet_scratch_gf_continue_2.pt')

    #   #lrs.append(scheduler.get_last_lr())

    print(f"Epoch : {epoch+1} Train_loss : {train_loss} Valid_loss : {valid_loss} Learning rate: ")
    logger.info(f"Epoch : {epoch+1} Train_loss : {train_loss} Valid_loss : {valid_loss} Learning rate: ") 
    counter +=1
    np.savez('losses.npz', train=train_losses, valid=valid_losses)







