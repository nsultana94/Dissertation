# -*- coding: utf-8 -*-
"""conv lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pk_9y1txgv_KprxWLG0C-PkaUjAF-E3J

# Installation
"""
import logging 

logging.basicConfig(filename="results.log", 
                    format='%(asctime)s %(message)s', 
                    filemode='w')

logger=logging.getLogger() 
logger.setLevel(logging.DEBUG) 
import torch 
import cv2

import numpy as np 
#import pandas as pd
import matplotlib.pyplot as plt 

from tqdm import tqdm

import os 
import copy
import glob
from torch import nn # neural netowrk 
import timm

import segmentation_models_pytorch as smp
from segmentation_models_pytorch.losses import DiceLoss, LovaszLoss, FocalLoss, JaccardLoss

from math import log

from torch import optim
import torchvision.models as models
import torch.nn.functional as f

from networks import SegmentationModel, ConvLSTMCell, LSTMModel, Initializer, Initializer2
from dataloader import get_train_augs, get_test_augs, get_valid_augs, SegmentationDataset, SegmentationDatasetUnet, get_test_augs_unet
from train_functions import train_function, eval_function
DEVICE = torch.device('cuda') 
# DEVICE = 'cuda' #Cuda as using GPU
import pandas as pd
from sklearn.metrics import jaccard_score, accuracy_score
import matplotlib
# from initialiser_train import UnetInitialiser

import logging 
logging.basicConfig(filename="std.log", 
                    format='%(asctime)s %(message)s', 
                    filemode='w')

logger=logging.getLogger() 
logger.setLevel(logging.DEBUG) 

EPOCHS = 50 #25 training iterations
LR = 0.00001 #decay learning rate
BATCH_SIZE = 4
HEIGHT = 288
WIDTH = 480
ENCODER = 'resnet34'
WEIGHTS = 'imagenet'
DATA_URL = "/cs/student/projects1/2019/nsultana/"

import random
training_images = (glob.glob(f"{DATA_URL}new_data/Training1/*.npz"))


testing_images = (glob.glob(f"{DATA_URL}new_data/Testing/*.npz"))

validation_images = (glob.glob(f"{DATA_URL}new_data/Validation/*.npz"))

import warnings
warnings.filterwarnings('always')  # "error", "ignore", "always", "default", "module" or "once"

"""# Set up model"""

import albumentations as A
from albumentations.pytorch import ToTensorV2

""" conv lstm code adapted from  https://github.com/ndrplz/ConvLSTM_pytorch/blob/master/convlstm.py """

class ConvLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers,
                 encoder, decoder, head):
        super(ConvLSTM, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        layers = []
        for i in range(0, self.num_layers):
            layers.append(ConvLSTMCell(input_size = self.input_size, hidden_size = self.hidden_size))

        self.layers = nn.ModuleList(layers)
        self.encoder = encoder
        self.decoder = decoder
        self.head = head

    def forward(self, images, masks = None):
        length = images.shape[1] - 2
        image_0 = images[:,0,:,:,:]
        image_1 = images[:,1,:,:,:]
        image_2 = images[:,2,:,:,:]
        if masks!=None:
            mask = masks[:,2,:,:]
            mask = mask.contiguous().long()
            ce_weights = calculate_weights(masks)
            ce_weights = torch.tensor(ce_weights,dtype=torch.float).to(DEVICE)


        fv1 = self.encoder(image_0)
        fv2 = self.encoder(image_1)
        x_tilda = self.encoder(image_2)



        c0 = fv1[5]
        h0 = fv2[5]
        feature_vectors = []
        x_tilda = self.encoder(image_2)
        feature_vector = x_tilda[5]
        
        layer_output_list = []
        x_tildas = []
        for i in range(0, length):
            
            x_tilda = self.encoder(images[:,i+2,:,:,:])
            feature_vectors.append(x_tilda[5])
            
            x_tildas.append(x_tilda)
            feature_vector = x_tilda[5]
            

        feature_vectors = torch.stack(feature_vectors)
        cur_layer_input = feature_vectors

        for layer_idx in range(self.num_layers):

            h = h0
            c = c0
            output_inner = []
            for i in range(0,length):
                
                h, c = self.layers[layer_idx](input_=cur_layer_input[i,:, :, :, :],
                                                    hiddenState=h, cellState=c)
                output_inner.append(h)
               

            layer_output = torch.stack(output_inner, dim=0)
            cur_layer_input = layer_output
            

        logits = []
        
        weights = [0.1, 0.25, 0.5, 0.25, 0.1]
        loss = 0
        logits = []
        for i in range(0, len(x_tildas)):
            
            h = cur_layer_input[i,:,:,:,:]
            x_tilda = x_tildas[i]
            x_tilda[5] = h
            decoder_output = self.decoder(*x_tilda)
            logits_mask = self.head(decoder_output)
            if masks!=None:
                mask = masks[:,i+2,:,:]
                mask = mask.contiguous()

                

                lovasz = LovaszLoss(mode = 'multiclass', ignore_index=-1)(logits_mask, mask)
            
                criterion = nn.CrossEntropyLoss(weight = ce_weights, ignore_index=-1)
                ce_logit = criterion(logits_mask, mask)
                loss = loss + (lovasz + ce_logit) * weights[i]
            logits.append(logits_mask)

            # hidden_loss = nn.MSELoss()
            # cell_loss = nn.MSELoss()

            # hidden_state_loss = hidden_loss(h, h0) 
            # cell_state_loss = cell_loss(c, c0)

        # loss = (lovasz_loss + cross_entropy_loss + (cell_state_loss) + hidden_state_loss)
        return loss, logits




model = SegmentationModel()
model.load_state_dict(torch.load(f'{DATA_URL}Models/best_model_aug.pt'))
model.to(DEVICE); #i.e CUDA

model_summary = model.show()
encoder = model_summary.encoder
initializer = Initializer()
decoder = model_summary.decoder
head = model_summary.segmentation_head

# convlstm  = ConvLSTMCell(input_size = 512, hidden_size = 512)
# new_model = LSTMModel(initializer,encoder,convlstm,decoder, head)
# new_model = new_model.to(device = DEVICE)

# new_model.load_state_dict(torch.load(f'{DATA_URL}Models/weightedce_025.pt'))

test_df = pd.read_csv(f"{DATA_URL}Data/Dataset/test_df_1.csv")

test_df = test_df.sort_values(by=['images'])

testing_images = sorted(testing_images)
testset = SegmentationDataset("Testing", get_test_augs(), testing_images)

# testset = SegmentationDatasetUnet(test_df, get_test_augs_unet())


initialiser = ConvLSTM(512, 512, 2, encoder, decoder, head)

initialiser = initialiser.to(device = DEVICE)

#initialiser.load_state_dict(torch.load(f'{DATA_URL}Models/U-net/4layers_finetuned.pt'))
print(len(test_df))



from torchvision.models.optical_flow import Raft_Small_Weights, Raft_Large_Weights
from torchvision.models.optical_flow import raft_small, raft_large

of_model = raft_large(weights=Raft_Large_Weights.DEFAULT, progress=False).to('cuda')
of_model = of_model.eval()





def applyOpticalFlow(of_model, images, s1):
    weights = Raft_Large_Weights.DEFAULT
    transforms = weights.transforms()
    img1_batch = torch.stack(list(images[0].unsqueeze(0)))
    img2_batch = torch.stack(list(images[1].unsqueeze(0)))
    img1_batch, img2_batch = transforms(img1_batch, img2_batch)
    with torch.no_grad():
        list_of_flows = of_model(img1_batch.to('cuda'), img2_batch.to('cuda'))
    predicted_flows = list_of_flows[-1]
    flow = predicted_flows[0]
    flow = flow.detach().to('cpu').numpy()
    flow = np.moveaxis(flow, 0, -1)

    flow = -flow

    h = flow.shape[0]
    w = flow.shape[1]

    flow[:,:,0] += np.arange(w)
    flow[:,:,1] += np.arange(h)[:,np.newaxis]
    s1 = s1.detach().cpu().numpy()
    new_frame = cv2.remap(s1, flow, None, interpolation=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT,  borderValue=100)
    new_frame[new_frame == 100] = -1
    return new_frame


# finding temporal consistency loss for initialiser network
preds = []
target = []
labels = [0,1,2,3,4,5,6,7]

import sklearn.metrics as skm
print("testing model 1")
def initialiseDictionary():
  labels = [0,1,2,3,4,5,6,7]
  label_stats = {}
  for label in labels:
    label_stats[label] = {'tp': 0, 'fn': 0, 'fp': 0, 'tn':0}
  return label_stats


stats =initialiseDictionary()

for idx in range (0, len(testset)):
  initialiser.eval()
  images, masks = testset[idx]
  images = images.to(device = DEVICE)
  masks = masks.to(device= DEVICE)


# final model 

  with torch.no_grad():
    loss, logits = initialiser(images.unsqueeze(0).to(DEVICE))
  predictions =  torch.nn.functional.softmax(logits[0].squeeze(0), dim=0)
  s1 = torch.argmax(predictions, dim=0)
  predictions =  torch.nn.functional.softmax(logits[1].squeeze(0), dim=0)
  s2 = torch.argmax(predictions, dim=0)

# unet 

#   model.eval()
#   images = images[2:]
#   with torch.no_grad():
#     logits_mask = model(images[0].unsqueeze(0).to(DEVICE))
#     predictions = torch.nn.functional.softmax(logits_mask, dim=1)
#     s1 = torch.argmax(predictions, dim=1).squeeze(0)
#     logits_mask = model(images[1].unsqueeze(0).to(DEVICE))
#     predictions = torch.nn.functional.softmax(logits_mask, dim=1)
#     s2 = torch.argmax(predictions, dim=1).squeeze(0)


  s3 = applyOpticalFlow(of_model, images, s1)

  target.append(s2.cpu().detach())
  preds.append(s3)
  prediction = s3.flatten()

  ground_truth = s2.cpu().detach().flatten()
  

  conf_matrix = skm.multilabel_confusion_matrix(ground_truth, prediction,labels=labels)
  for label in labels:
    stats[label]['tp'] += conf_matrix[label][1][1] 
    stats[label]['fn'] += conf_matrix[label][1][0] 
    stats[label]['fp'] += conf_matrix[label][0][1]
    stats[label]['tn'] += conf_matrix[label][0][0]


miou = 0
for label in labels:
    tp = stats[label]['tp'] 
    fn = stats[label]['fn'] 
    fp = stats[label]['fp'] 
    tn = stats[label]['tn'] 
    iou = tp / ( fp + tp + fn)
    pa = (tp + tn) / ( fp + tp + fn + tn)
    miou+=iou
    print(f"class {label} iou: {iou} pa: {pa}")
miou = miou / len(labels)
print(f"miou : {miou}")


from torchmetrics.classification import MulticlassJaccardIndex

# metric = MulticlassJaccardIndex(num_classes=8, average = None)

# target = np.stack(target)
# preds = np.stack(preds)
# preds = torch.Tensor(preds)
# target = torch.Tensor(target)


# print(metric(preds, target))
    






